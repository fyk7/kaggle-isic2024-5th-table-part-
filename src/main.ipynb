{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "DLrjZ_YWd85d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLrjZ_YWd85d",
        "outputId": "1e2d4d23-f3ad-4e73-daa3-c34e61aff5fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Q2Tk9sUfihMf",
      "metadata": {
        "id": "Q2Tk9sUfihMf"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "U0mlduEne2-8",
      "metadata": {
        "id": "U0mlduEne2-8"
      },
      "outputs": [],
      "source": [
        "!pip install -U polars optuna catboost >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "52a-yynFEhW4",
      "metadata": {
        "id": "52a-yynFEhW4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5ae969f9-9447-4809-b852-62e8197cbe44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ae969f9-9447-4809-b852-62e8197cbe44",
        "outputId": "91a21f15-c2d9-4988-d581-a9c9bf708281"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import seaborn as sns\n",
        "import optuna\n",
        "\n",
        "# カラム数の制限を解除\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "# 行数の制限を解除\n",
        "pd.set_option('display.max_rows', 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "qxlw2A3pcUC3",
      "metadata": {
        "id": "qxlw2A3pcUC3"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = Path('/content/drive/MyDrive/kaggle/isic2024')\n",
        "OUTPUT_DIR = BASE_PATH / 'output'\n",
        "DATA_PATH = BASE_PATH / 'data'\n",
        "TRAIN_PATH = DATA_PATH /'train-metadata.csv'\n",
        "\n",
        "TARGET_COL = 'target'\n",
        "ERR = 1e-5\n",
        "N_SPLITS = 5\n",
        "SPLIT_SEED = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Rvcv9nZKixUe",
      "metadata": {
        "id": "Rvcv9nZKixUe"
      },
      "outputs": [],
      "source": [
        "# register settings like hyper prams to CFG class from Google Spread Sheet\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc_auth = gspread.authorize(creds)\n",
        "\n",
        "# Please set your Google Spread Sheet url below.\n",
        "url = \"\"\n",
        "ss = gc_auth.open_by_url(url)\n",
        "param_sheet = ss.worksheet(\"tree_model_params_v3\")\n",
        "df_param = get_as_dataframe(param_sheet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ML8gjb0Ui0Ik",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML8gjb0Ui0Ik",
        "outputId": "daf164fa-f570-424e-c2c3-9ac5c4035464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exp_ver=1 img_cols=['pred_sub_71', 'pred_sub_73', 'pred_sub_75', 'pred_sub_77', 'pred_tsuma_eva', 'pred_tsuma_conv_nes'] attribution_flag=0 feature_select_flag=1 lesion_id_weight=0.0 Indeterminate_weight=0.0\n",
            "exp_ver=2 img_cols=['pred_sub_71', 'pred_sub_73', 'pred_sub_75', 'pred_sub_77', 'pred_tsuma_eva'] attribution_flag=0 feature_select_flag=1 lesion_id_weight=0.0 Indeterminate_weight=0.0\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "# Final submission Patterns\n",
        "# all_img_cols = [\n",
        "#     'pred_sub_71',\n",
        "#     'pred_sub_73',\n",
        "#     'pred_sub_75',\n",
        "#     'pred_sub_77',\n",
        "#     'pred_tsuma_eva_nes',\n",
        "#     'pred_tsuma_conv_nes',\n",
        "#     'pred_sub_71,pred_sub_73,pred_sub_75,pred_tsuma_eva_nes',\n",
        "#     'pred_sub_77,pred_sub_75,pred_tsuma_conv_nes',\n",
        "#     'pred_sub_77,pred_sub_75,pred_tsuma_conv_nes,pred_tsuma_eva_nes',\n",
        "#     'pred_sub_71,pred_sub_73,pred_sub_75,pred_sub_77,pred_tsuma_eva,pred_tsuma_conv_nes'\n",
        "# ]\n",
        "# all_attribution_flags = [0, 1]\n",
        "# all_feature_select_flags = [0, 1]\n",
        "# all_lesion_id_weights = [0.0]\n",
        "# all_Indeterminate_weights = [0.0, 0.5]\n",
        "\n",
        "\n",
        "# Test patterns\n",
        "all_img_cols = [\n",
        "    'pred_sub_71,pred_sub_73,pred_sub_75,pred_sub_77,pred_tsuma_eva,pred_tsuma_conv_nes',\n",
        "    'pred_sub_71,pred_sub_73,pred_sub_75,pred_sub_77,pred_tsuma_eva',\n",
        "]\n",
        "all_attribution_flags = [0]\n",
        "all_feature_select_flags = [1]\n",
        "all_lesion_id_weights = [0.0]\n",
        "all_Indeterminate_weights = [0]\n",
        "\n",
        "class CFG(BaseModel):\n",
        "    exp_ver: Optional[str] = \"32\"\n",
        "    img_cols: Optional[str] = \"pred_sub_71,pred_sub_73,pred_sub_75\"\n",
        "    attribution_flag: Optional[int] = 1\n",
        "    feature_select_flag: Optional[int] = 1\n",
        "    lesion_id_weight: Optional[float] = 0.0\n",
        "    Indeterminate_weight: Optional[float] = 0.5\n",
        "\n",
        "\n",
        "all_config_list = []\n",
        "counter = 0\n",
        "start_from = 1\n",
        "restart_from = 1\n",
        "for img_col in all_img_cols:\n",
        "    for attribution_flag in all_attribution_flags:\n",
        "        for feature_select_flag in all_feature_select_flags:\n",
        "            for lesion_id_weight in all_lesion_id_weights:\n",
        "                for Indeterminate_weight in all_Indeterminate_weights:\n",
        "                    cfg = CFG()\n",
        "                    cfg.exp_ver = start_from+counter\n",
        "                    if cfg.exp_ver < restart_from:\n",
        "                        continue\n",
        "                    cfg.img_cols = img_col.split(',')\n",
        "                    cfg.attribution_flag = int(attribution_flag)\n",
        "                    cfg.feature_select_flag = int(feature_select_flag)\n",
        "                    cfg.lesion_id_weight = float(lesion_id_weight)\n",
        "                    cfg.Indeterminate_weight = float(Indeterminate_weight)\n",
        "\n",
        "                    param_sheet.update_acell(f\"A{cfg.exp_ver+1}\", cfg.exp_ver)\n",
        "                    param_sheet.update_acell(f\"B{cfg.exp_ver+1}\", img_col)\n",
        "                    param_sheet.update_acell(f\"C{cfg.exp_ver+1}\", attribution_flag)\n",
        "                    param_sheet.update_acell(f\"D{cfg.exp_ver+1}\", feature_select_flag)\n",
        "                    param_sheet.update_acell(f\"E{cfg.exp_ver+1}\", lesion_id_weight)\n",
        "                    param_sheet.update_acell(f\"F{cfg.exp_ver+1}\", Indeterminate_weight)\n",
        "                    all_config_list.append(cfg)\n",
        "                    counter += 1\n",
        "for cfg in all_config_list:\n",
        "    print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "IZRkgDaqarCw",
      "metadata": {
        "id": "IZRkgDaqarCw"
      },
      "outputs": [],
      "source": [
        "def custom_metric(estimator, X, y_true):\n",
        "    y_hat = estimator.predict_proba(X)[:, 1]\n",
        "    min_tpr = 0.80\n",
        "    max_fpr = abs(1 - min_tpr)\n",
        "\n",
        "    v_gt = abs(y_true - 1)\n",
        "    v_pred = np.array([1.0 - x for x in y_hat])\n",
        "\n",
        "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
        "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
        "\n",
        "    return partial_auc\n",
        "\n",
        "\n",
        "def score(solution: np.ndarray, submission: np.ndarray, min_tpr: float=0.80) -> float:\n",
        "    v_gt = abs(solution-1)\n",
        "    v_pred = np.array([1.0 - x for x in submission])\n",
        "    max_fpr = abs(1-min_tpr)\n",
        "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
        "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
        "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
        "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
        "\n",
        "    return(partial_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9656fc44-3159-4279-bfe2-083315729c4d",
      "metadata": {
        "id": "9656fc44-3159-4279-bfe2-083315729c4d"
      },
      "outputs": [],
      "source": [
        "num_cols = [\n",
        "    'age_approx',                        # Approximate age of patient at time of imaging.\n",
        "    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n",
        "    'tbp_lv_A',                          # A inside  lesion.+\n",
        "    'tbp_lv_Aext',                       # A outside lesion.+\n",
        "    'tbp_lv_B',                          # B inside  lesion.+\n",
        "    'tbp_lv_Bext',                       # B outside lesion.+\n",
        "    'tbp_lv_C',                          # Chroma inside  lesion.+\n",
        "    'tbp_lv_Cext',                       # Chroma outside lesion.+\n",
        "    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n",
        "    'tbp_lv_Hext',                       # Hue outside lesion.+\n",
        "    'tbp_lv_L',                          # L inside lesion.+\n",
        "    'tbp_lv_Lext',                       # L outside lesion.+\n",
        "    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n",
        "    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n",
        "    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n",
        "    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n",
        "    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n",
        "    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n",
        "    'tbp_lv_deltaLB',                    #\n",
        "    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n",
        "    'tbp_lv_eccentricity',               # Eccentricity.+\n",
        "    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n",
        "    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n",
        "    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n",
        "    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n",
        "    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n",
        "    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n",
        "    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n",
        "    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n",
        "    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n",
        "    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n",
        "    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n",
        "    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n",
        "    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n",
        "]\n",
        "\n",
        "new_num_cols = [\n",
        "    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n",
        "    'lesion_size_ratio_with_area',   # tbp_lv_minorAxisMM      / clin_size_long_diam_mm * tbp_lv_areaMM2\n",
        "    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n",
        "    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs\n",
        "    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs\n",
        "    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt\n",
        "    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n",
        "    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n",
        "\n",
        "    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n",
        "    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n",
        "    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n",
        "    'lesion_visibility_score',          # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n",
        "    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n",
        "    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n",
        "\n",
        "    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext\n",
        "    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n",
        "    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx\n",
        "    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean\n",
        "    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n",
        "    'shape_complexity_index',        # border_complexity       + lesion_shape_index\n",
        "    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n",
        "\n",
        "    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log\n",
        "    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx\n",
        "    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2\n",
        "    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n",
        "    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n",
        "    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2\n",
        "    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n",
        "\n",
        "    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n",
        "    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n",
        "    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n",
        "    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color\n",
        "    'border_color_interaction_2',\n",
        "    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n",
        "    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx\n",
        "    'age_normalized_nevi_confidence_2',\n",
        "    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n",
        "\n",
        "    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n",
        "    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n",
        "    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n",
        "    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n",
        "    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n",
        "    'index_age_size_symmetry',       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n",
        "\n",
        "    'border_mul_perimeter',\n",
        "]\n",
        "\n",
        "cat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution', 'copyright_license']\n",
        "\n",
        "norm_cols1 = [f'{col}_patient_norm' for col in num_cols + new_num_cols]\n",
        "norm_cols2 = [f'{col}_attribution_norm' for col in num_cols + new_num_cols]\n",
        "norm_cols3 = [f'{col}_patient_lv_location_norm' for col in num_cols + new_num_cols]\n",
        "norm_cols4 = [f'{col}_attribution_lv_location_norm' for col in num_cols + new_num_cols]\n",
        "norm_cols5 = [f'{col}_lv_location_norm' for col in num_cols + new_num_cols]\n",
        "norm_cols6 = [f'{col}_attribution_lv_location_tile_type_norm' for col in num_cols + new_num_cols]\n",
        "norm_cols7 = [f'{col}_patient_lv_location_tile_type_norm' for col in num_cols + new_num_cols]\n",
        "\n",
        "agg_list = ['mean', 'max', 'sum', 'std']\n",
        "additinal_features1 = [f'{col}_patient_{agg}' for col in num_cols + new_num_cols for agg in agg_list]\n",
        "additinal_features2 = [f'{col}_patient_lv_location_{agg}' for col in num_cols + new_num_cols for agg in agg_list]\n",
        "additinal_features3 = [f'{col}_patient_lv_location_tile_type_{agg}' for col in num_cols + new_num_cols for agg in agg_list]\n",
        "additinal_features4 = [f'{col}_attribution_{agg}' for col in num_cols + new_num_cols for agg in agg_list]\n",
        "additinal_features5 = [f'{col}_attribution_lv_location_{agg}' for col in num_cols + new_num_cols for agg in agg_list]\n",
        "\n",
        "cnt_features = ['isic_id_count', 'isic_id_count_patient_lv_location', 'isic_id_count_attribution', 'isic_id_count_attribution_lv_location']\n",
        "age_features = ['unique_age_approx_count', 'age_min_diff', 'age_max_diff', 'age_min_max', 'age_range', 'age_phase']\n",
        "\n",
        "pivot_cnt_features = [\n",
        " 'anatom_site_general_anterior torso_count',\n",
        " 'anatom_site_general_upper extremity_count',\n",
        " 'anatom_site_general_lower extremity_count',\n",
        " 'anatom_site_general_posterior torso_count',\n",
        " 'anatom_site_general_head/neck_count',\n",
        " 'anatom_site_general__count',\n",
        " 'combined_anatomical_site_lower extremity_Left Leg - Lower_count',\n",
        " 'combined_anatomical_site_anterior torso_Torso Front Top Half_count',\n",
        " 'combined_anatomical_site_upper extremity_Right Arm - Lower_count',\n",
        " 'combined_anatomical_site_upper extremity_Left Arm - Lower_count',\n",
        " 'combined_anatomical_site_head/neck_Head & Neck_count',\n",
        " 'combined_anatomical_site_anterior torso_Torso Front Bottom Half_count',\n",
        " 'combined_anatomical_site_lower extremity_Right Leg - Lower_count',\n",
        " 'combined_anatomical_site_posterior torso_Torso Back Bottom Third_count',\n",
        " 'combined_anatomical_site_upper extremity_Right Arm - Upper_count',\n",
        " 'combined_anatomical_site_lower extremity_Right Leg - Upper_count',\n",
        " 'combined_anatomical_site_posterior torso_Torso Back Top Third_count',\n",
        " 'combined_anatomical_site_upper extremity_Left Arm - Upper_count',\n",
        " 'combined_anatomical_site_lower extremity_Left Leg - Upper_count',\n",
        " 'combined_anatomical_site_posterior torso_Torso Back Middle Third_count',\n",
        " 'combined_anatomical_site_upper extremity_Left Arm_count',\n",
        " 'combined_anatomical_site_lower extremity_Right Leg_count',\n",
        " 'combined_anatomical_site_lower extremity_Left Leg_count',\n",
        " 'combined_anatomical_site_upper extremity_Right Arm_count',\n",
        " 'combined_anatomical_site__Unknown_count',\n",
        " 'combined_anatomical_site_posterior torso_Torso Back_count',\n",
        " 'combined_anatomical_site_anterior torso_Torso Front_count',\n",
        " 'tbp_lv_location_Left Leg - Upper_count',\n",
        " 'tbp_lv_location_Right Arm - Lower_count',\n",
        " 'tbp_lv_location_Right Leg - Upper_count',\n",
        " 'tbp_lv_location_Torso Back Bottom Third_count',\n",
        " 'tbp_lv_location_Left Arm - Upper_count',\n",
        " 'tbp_lv_location_Torso Front Top Half_count',\n",
        " 'tbp_lv_location_Torso Back Middle Third_count',\n",
        " 'tbp_lv_location_Torso Front Bottom Half_count',\n",
        " 'tbp_lv_location_Torso Back Top Third_count',\n",
        " 'tbp_lv_location_Right Arm - Upper_count',\n",
        " 'tbp_lv_location_Head & Neck_count',\n",
        " 'tbp_lv_location_Left Arm_count',\n",
        " 'tbp_lv_location_Left Arm - Lower_count',\n",
        " 'tbp_lv_location_Right Leg - Lower_count',\n",
        " 'tbp_lv_location_Left Leg - Lower_count',\n",
        " 'tbp_lv_location_Torso Front_count',\n",
        " 'tbp_lv_location_Left Leg_count',\n",
        " 'tbp_lv_location_Right Arm_count',\n",
        " 'tbp_lv_location_Right Leg_count',\n",
        " 'tbp_lv_location_Unknown_count',\n",
        " 'tbp_lv_location_Torso Back_count'\n",
        "]\n",
        "\n",
        "feature_cols = (\n",
        "    num_cols +\n",
        "    new_num_cols +\n",
        "    cat_cols +\n",
        "    norm_cols1 +\n",
        "    norm_cols2 +\n",
        "    norm_cols3 +\n",
        "    norm_cols4 +\n",
        "    norm_cols5 +\n",
        "    norm_cols6 +\n",
        "    additinal_features1 +\n",
        "    additinal_features2 +\n",
        "    additinal_features4 +\n",
        "    additinal_features5 +\n",
        "    cnt_features +\n",
        "    age_features +\n",
        "    pivot_cnt_features\n",
        "    #\n",
        "    # +image_cols\n",
        ")\n",
        "\n",
        "\n",
        "def read_data(path):\n",
        "    return (\n",
        "        pl.read_csv(path)\n",
        "        .with_columns(\n",
        "            pl.col('age_approx').cast(pl.Utf8).replace('NA', np.nan).cast(pl.Float64),\n",
        "        )\n",
        "        .with_columns(\n",
        "            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n",
        "        )\n",
        "        .with_columns(\n",
        "            lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n",
        "            lesion_size_ratio_with_area    = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_areaMM2'),\n",
        "            lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n",
        "            # hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n",
        "            # luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n",
        "            hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')), # not use abs\n",
        "            luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')), # not use abs\n",
        "            lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n",
        "            border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n",
        "            border_mul_perimeter          =  pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_perimeterMM'),\n",
        "            color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + ERR),\n",
        "        )\n",
        "        .with_columns(\n",
        "            position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n",
        "            perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n",
        "            area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n",
        "            lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n",
        "            combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n",
        "            symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n",
        "            consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n",
        "        )\n",
        "        .with_columns(\n",
        "            color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n",
        "            consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n",
        "            size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n",
        "            hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n",
        "            lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n",
        "            shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n",
        "            color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n",
        "        )\n",
        "        .with_columns(\n",
        "            log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n",
        "            normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n",
        "            mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n",
        "            std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n",
        "            color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n",
        "            lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n",
        "            overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n",
        "        )\n",
        "        .with_columns(\n",
        "            symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n",
        "            comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n",
        "            color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n",
        "            border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n",
        "            border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n",
        "            size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n",
        "            age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n",
        "            age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n",
        "            color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n",
        "        )\n",
        "        .with_columns(\n",
        "            volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n",
        "            color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n",
        "            shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n",
        "            border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n",
        "            age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n",
        "            index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n",
        "        )\n",
        "        .with_columns(\n",
        "            [\n",
        "                pl.concat_str([pl.col(col) for col in combo], separator=\"_\").alias(\"_\".join(combo))\n",
        "                for r in range(2, 4)\n",
        "                for combo in combinations(['sex', 'tbp_tile_type', 'tbp_lv_location', 'attribution', 'copyright_license'], r)\n",
        "            ]\n",
        "        )\n",
        "        # The degree to which each lesion is separated from the relevant lesion in each patient (with normalization).\n",
        "        .with_columns(\n",
        "            (\n",
        "                (pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + ERR)\n",
        "            ).alias(f'{col}_patient_norm')\n",
        "            for col in (num_cols + new_num_cols)\n",
        "        )\n",
        "        # The degree to which the relevant lesion is separated from others within each hospital's lesions (with normalization).\n",
        "        .with_columns(\n",
        "            (\n",
        "                (pl.col(col) - pl.col(col).mean().over(['attribution'])) / (pl.col(col).std().over(['attribution']) + ERR)\n",
        "            ).alias(f'{col}_attribution_norm')\n",
        "            for col in (num_cols + new_num_cols)\n",
        "        )\n",
        "        # The degree to which the relevant lesion is separated from others within each tbp_lv_location's lesions (with normalization).\n",
        "        .with_columns(\n",
        "            (\n",
        "                (pl.col(col) - pl.col(col).mean().over(['tbp_lv_location'])) / (pl.col(col).std().over(['tbp_lv_location']) + ERR)\n",
        "            ).alias(f'{col}_lv_location_norm')\n",
        "            for col in (num_cols + new_num_cols)\n",
        "        )\n",
        "        .with_columns(\n",
        "            (\n",
        "                (pl.col(col) - pl.col(col).mean().over(['patient_id', 'tbp_lv_location'])) / (pl.col(col).std().over(['patient_id', 'tbp_lv_location']) + ERR)\n",
        "            ).alias(f'{col}_patient_lv_location_norm')\n",
        "            for col in (num_cols + new_num_cols)\n",
        "        )\n",
        "        # The degree to which the relevant lesion is separated from others within each hospital and site's lesions (with normalization).\n",
        "        # 各病院・部位のlesion中で該当lesionがどの程度かけ離れているか\n",
        "        .with_columns(\n",
        "            (\n",
        "                (pl.col(col) - pl.col(col).mean().over(['attribution', 'tbp_lv_location'])) / (pl.col(col).std().over(['attribution', 'tbp_lv_location']) + ERR)\n",
        "            ).alias(f'{col}_attribution_lv_location_norm')\n",
        "            for col in (num_cols + new_num_cols)\n",
        "        )\n",
        "        # 各病院・部位・tile_typeのlesion中で該当lesionがどの程度かけ離れているか\n",
        "        .with_columns(\n",
        "            (\n",
        "                (pl.col(col) - pl.col(col).mean().over(['attribution', 'tbp_lv_location', 'tbp_tile_type'])) / (pl.col(col).std().over(['attribution', 'tbp_lv_location', 'tbp_tile_type']) + ERR)\n",
        "            ).alias(f'{col}_attribution_lv_location_tile_type_norm')\n",
        "            for col in (num_cols + new_num_cols)\n",
        "        )\n",
        "        .with_columns(\n",
        "            pl.col(cat_cols).cast(pl.Categorical),\n",
        "        )\n",
        "    )\n",
        "\n",
        "def feature_engineering_patient_id_pl(df: pl.DataFrame, num_cols: list[str]) -> pl.DataFrame:\n",
        "    # 数値系変数の集計特徴量を計算\n",
        "    agg_numeric = (\n",
        "        df.group_by('patient_id')\n",
        "        .agg([\n",
        "            pl.col(col).mean().alias(f'{col}_patient_mean') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).std().alias(f'{col}_patient_std') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).max().alias(f'{col}_patient_max') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).sum().alias(f'{col}_patient_sum') for col in num_cols\n",
        "        ] + [\n",
        "            pl.count('isic_id').alias('isic_id_count')\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    agg_numeric2 = (\n",
        "        df.group_by(['patient_id', 'tbp_lv_location'])\n",
        "        .agg([\n",
        "            pl.col(col).mean().alias(f'{col}_patient_lv_location_mean') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).std().alias(f'{col}_patient_lv_location_std') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).max().alias(f'{col}_patient_lv_location_max') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).sum().alias(f'{col}_patient_lv_location_sum') for col in num_cols\n",
        "        ] + [\n",
        "            pl.count('isic_id').alias('isic_id_count_patient_lv_location')\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    agg_numeric4 = (\n",
        "        df.group_by(['attribution'])\n",
        "        .agg([\n",
        "            pl.col(col).mean().alias(f'{col}_attribution_mean') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).std().alias(f'{col}_attribution_std') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).max().alias(f'{col}_attribution_max') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).sum().alias(f'{col}_attribution_sum') for col in num_cols\n",
        "        ] + [\n",
        "            pl.count('isic_id').alias('isic_id_count_attribution')\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    agg_numeric5 = (\n",
        "        df.group_by(['attribution', 'tbp_lv_location'])\n",
        "        .agg([\n",
        "            pl.col(col).mean().alias(f'{col}_attribution_lv_location_mean') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).std().alias(f'{col}_attribution_lv_location_std') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).max().alias(f'{col}_attribution_lv_location_max') for col in num_cols\n",
        "        ] + [\n",
        "            pl.col(col).sum().alias(f'{col}_attribution_lv_location_sum') for col in num_cols\n",
        "        ] + [\n",
        "            pl.count('isic_id').alias('isic_id_count_attribution_lv_location')\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    categorical_columns = ['anatom_site_general', 'combined_anatomical_site', \"tbp_lv_location\"]\n",
        "    for col in categorical_columns:\n",
        "        counts = (\n",
        "            df.group_by(['patient_id', col])\n",
        "            .agg(pl.count().alias(f'{col}_count'))\n",
        "            .pivot(\n",
        "                index='patient_id',\n",
        "                columns=col,\n",
        "                values=f'{col}_count',\n",
        "                aggregate_function='sum'\n",
        "            )\n",
        "            .fill_null(0)\n",
        "        )\n",
        "        col_names = [f'{col}_{val}_count' for val in counts.columns if val != 'patient_id']\n",
        "        counts = counts.rename({old: new for old, new in zip(counts.columns, ['patient_id'] + col_names)})\n",
        "        df = df.join(counts, on='patient_id', how='left')\n",
        "\n",
        "\n",
        "    df = df.join(agg_numeric, on='patient_id', how='left')\n",
        "    df = df.join(agg_numeric2, on=['patient_id', 'tbp_lv_location'], how='left')\n",
        "    df = df.join(agg_numeric4, on=['attribution'], how='left')\n",
        "    df = df.join(agg_numeric5, on=['attribution', 'tbp_lv_location'], how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "def feature_engineering_age_pl(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    unique_counts = (\n",
        "        df.group_by('patient_id')\n",
        "        .agg(\n",
        "            pl.col('age_approx').n_unique().alias('unique_age_approx_count')\n",
        "        )\n",
        "    )\n",
        "    df = df.join(unique_counts, on='patient_id', how='left')\n",
        "    df = df.with_columns(\n",
        "        age_min_diff = pl.col('age_approx') - pl.col('age_approx').min().over('patient_id'),\n",
        "        age_max_diff = pl.col('age_approx') - pl.col('age_approx').max().over('patient_id'),\n",
        "        age_min_max = (pl.col('age_approx') - pl.col('age_approx').min().over('patient_id')) +\n",
        "                         (pl.col('age_approx') - pl.col('age_approx').max().over('patient_id')),\n",
        "        age_range = pl.col('age_approx').max().over('patient_id') - pl.col('age_approx').min().over('patient_id'),\n",
        "        age_phase = pl.col('age_approx') / (pl.col('age_approx').max().over('patient_id') - pl.col('age_approx').min().over('patient_id') + ERR),\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_metadata(past_meta, meta, num_cols):\n",
        "    # datatype optimization\n",
        "    def optimize_dataframe(df):\n",
        "        df = df.with_columns([\n",
        "            pl.col(pl.Float64).cast(pl.Float32),\n",
        "            pl.col(pl.Int64).cast(pl.Int32)\n",
        "        ])\n",
        "        return df\n",
        "\n",
        "    # skin_type from past metadata\n",
        "    skin_map = {\n",
        "        'I': 0,\n",
        "        'II': 1,\n",
        "        'III': 2,\n",
        "        'IV': 3,\n",
        "    }\n",
        "    mapped_col = (\n",
        "        pl.when(pl.col('fitzpatrick_skin_type') == 'I').then(0)\n",
        "        .when(pl.col('fitzpatrick_skin_type') == 'II').then(1)\n",
        "        .when(pl.col('fitzpatrick_skin_type') == 'III').then(2)\n",
        "        .when(pl.col('fitzpatrick_skin_type') == 'IV').then(3)\n",
        "        .otherwise(None).alias('fitzpatrick_skin_type_mapped')\n",
        "    )\n",
        "    past_meta = past_meta.with_columns([mapped_col])\n",
        "    skin_df = (\n",
        "        past_meta.group_by('patient_id')\n",
        "        .agg(pl.col('fitzpatrick_skin_type_mapped').min().alias('fitzpatrick_skin_type_mapped'))\n",
        "        .filter(pl.col('fitzpatrick_skin_type_mapped').is_not_null())\n",
        "    )\n",
        "    meta = meta.join(skin_df, on='patient_id', how='left')\n",
        "    skin_df_cols = skin_df.columns\n",
        "    del skin_df\n",
        "    gc.collect()\n",
        "\n",
        "    # ピボットとグループ化の関数\n",
        "    def pivot_group_count(df, groupby_cols, pivot_index, pivot_column, count_column):\n",
        "        group_count = (\n",
        "            df.group_by(groupby_cols)\n",
        "            .agg(pl.col(count_column).count().alias(count_column))\n",
        "        )\n",
        "        pivot_df = group_count.pivot(\n",
        "            index=pivot_index,\n",
        "            columns=pivot_column,\n",
        "            values=count_column\n",
        "        )\n",
        "        return pivot_df\n",
        "\n",
        "    pivot_df_1 = pivot_group_count(\n",
        "        df=past_meta,\n",
        "        groupby_cols=['patient_id', 'anatom_site_general', 'age_approx',  'benign_malignant'],\n",
        "        pivot_index=['patient_id', 'anatom_site_general', 'age_approx'],\n",
        "        pivot_column='benign_malignant',\n",
        "        count_column='isic_id'\n",
        "    )\n",
        "\n",
        "    # データ型を一致させる\n",
        "    pivot_df_1 = pivot_df_1.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "        pl.col('age_approx').cast(pl.Float64)\n",
        "    ])\n",
        "\n",
        "    meta = meta.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "        pl.col('age_approx').cast(pl.Float64)\n",
        "    ])\n",
        "\n",
        "    meta = meta.join(pivot_df_1, on=['patient_id', 'anatom_site_general', 'age_approx'], how='left')\n",
        "    pivot_df_1_cols = pivot_df_1.columns\n",
        "    del pivot_df_1\n",
        "    gc.collect()\n",
        "\n",
        "    pivot_df_2 = pivot_group_count(\n",
        "        df=past_meta,\n",
        "        groupby_cols=['patient_id', 'anatom_site_general', 'benign_malignant'],\n",
        "        pivot_index=['patient_id', 'anatom_site_general'],\n",
        "        pivot_column='benign_malignant',\n",
        "        count_column='isic_id'\n",
        "    )\n",
        "\n",
        "    # データ型を一致させる\n",
        "    pivot_df_2 = pivot_df_2.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "        # pl.col('age_approx').cast(pl.Float64)\n",
        "    ])\n",
        "\n",
        "    meta = meta.join(pivot_df_2, on=['patient_id', 'anatom_site_general'], how='left')\n",
        "    pivot_df_2_cols = pivot_df_2.columns\n",
        "    del pivot_df_2\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    benign_malignant_map = {\n",
        "        'benign': 0,\n",
        "        'indeterminate': 0,\n",
        "        'indeterminate/benign': 0,\n",
        "        'indeterminate/malignant': 1,\n",
        "        'malignant': 1,\n",
        "    }\n",
        "    mapped_target_col = (\n",
        "        pl.when(pl.col('benign_malignant') == 'benign').then(0)\n",
        "        .when(pl.col('benign_malignant') == 'indeterminate').then(0)\n",
        "        .when(pl.col('benign_malignant') == 'indeterminate/benign').then(0)\n",
        "        .when(pl.col('benign_malignant') == 'indeterminate/malignant').then(1)\n",
        "        .when(pl.col('benign_malignant') == 'malignant').then(1)\n",
        "        .otherwise(None).alias('binary_target')\n",
        "    )\n",
        "    past_meta = past_meta.with_columns([mapped_target_col])\n",
        "\n",
        "    def calculate_group_aggregates(df, groupby_columns, columns_to_aggregate, aggregations, suffix):\n",
        "        agg_df = df.group_by(groupby_columns).agg([\n",
        "            getattr(pl.col(column), agg)().alias(f'{alias}_{agg}_{suffix}')\n",
        "            for column, alias in columns_to_aggregate.items()\n",
        "            for agg in aggregations\n",
        "        ])\n",
        "        agg_df = optimize_dataframe(agg_df)\n",
        "        return agg_df, agg_df.columns\n",
        "\n",
        "    columns_to_aggregate = {\n",
        "        'binary_target': 'binary_target_agg',\n",
        "        'clin_size_long_diam_mm': 'clin_size_long_diam_mm_agg'\n",
        "    }\n",
        "    aggregations = ['mean', 'max']\n",
        "\n",
        "    patient_aggregates, patient_aggregates_cols = calculate_group_aggregates(\n",
        "        past_meta,\n",
        "        ['patient_id'],\n",
        "        columns_to_aggregate,\n",
        "        aggregations,\n",
        "        '_p'\n",
        "    )\n",
        "    meta = meta.join(patient_aggregates, on=['patient_id'], how='left')\n",
        "    del patient_aggregates\n",
        "    gc.collect()\n",
        "\n",
        "    patient_age_aggregates, patient_age_aggregates_cols = calculate_group_aggregates(\n",
        "        past_meta,\n",
        "        ['patient_id', 'anatom_site_general'],\n",
        "        columns_to_aggregate,\n",
        "        aggregations,\n",
        "        '_ps',\n",
        "    )\n",
        "    patient_age_aggregates = patient_age_aggregates.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "    ])\n",
        "    meta = meta.join(patient_age_aggregates, on=['patient_id', 'anatom_site_general'], how='left')\n",
        "    del patient_age_aggregates\n",
        "    gc.collect()\n",
        "\n",
        "    patient_age_site_aggregates, patient_age_site_aggregates_cols = calculate_group_aggregates(\n",
        "        past_meta,\n",
        "        ['patient_id', 'anatom_site_general', 'age_approx'],\n",
        "        columns_to_aggregate,\n",
        "        aggregations,\n",
        "        '_psa',\n",
        "    )\n",
        "    patient_age_site_aggregates = patient_age_site_aggregates.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "        pl.col('age_approx').cast(pl.Float64)\n",
        "    ])\n",
        "    meta = meta.join(patient_age_site_aggregates, on=['patient_id', 'anatom_site_general', 'age_approx'], how='left')\n",
        "    del patient_age_site_aggregates\n",
        "    gc.collect()\n",
        "\n",
        "    # Shift Features\n",
        "    # num features from the past and future for the same patient and anatom_site_general.\n",
        "    # But in real world we shouldn't user 'future one...'\n",
        "    aggregations = ['mean', 'max']\n",
        "    meta_g = (\n",
        "        meta.group_by(['patient_id', 'anatom_site_general', 'age_approx'], maintain_order=True)\n",
        "        .agg([\n",
        "            getattr(pl.col(col), agg)().alias(f'{col}_{agg}')\n",
        "            for col in num_cols\n",
        "            for agg in aggregations\n",
        "        ])\n",
        "    )\n",
        "    meta_g = optimize_dataframe(meta_g)\n",
        "\n",
        "    shift_range = 5\n",
        "    new_feature_names = []\n",
        "    for shift in range(1, shift_range + 1):\n",
        "        for agg in aggregations:\n",
        "            for col in num_cols:\n",
        "                base_col = f'{col}_{agg}'\n",
        "\n",
        "                # In real world we shouldn't use 'next' shift features.\n",
        "                prev_col = f'{base_col}_prev_{shift}'\n",
        "                next_col = f'{base_col}_next_{shift}'\n",
        "\n",
        "                meta_g = meta_g.with_columns([\n",
        "                    pl.col(base_col).shift(shift).over(['patient_id', 'anatom_site_general']).alias(prev_col),\n",
        "                    pl.col(base_col).shift(-shift).over(['patient_id', 'anatom_site_general']).alias(next_col)\n",
        "                ])\n",
        "                meta_g = meta_g.with_columns([\n",
        "                    (pl.col(base_col) - pl.col(prev_col)).alias(f'{base_col}_diff_prev_{shift}'),\n",
        "                    (pl.col(base_col) - pl.col(next_col)).alias(f'{base_col}_diff_next_{shift}')\n",
        "                ])\n",
        "\n",
        "                new_feature_names.extend([prev_col, next_col, f'{base_col}_diff_prev_{shift}', f'{base_col}_diff_next_{shift}'])\n",
        "\n",
        "    meta_g = meta_g.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "        pl.col('age_approx').cast(pl.Float64)\n",
        "    ])\n",
        "    meta = meta.join(meta_g, on=['patient_id', 'anatom_site_general', 'age_approx'], how='left')\n",
        "\n",
        "    del meta_g\n",
        "    gc.collect()\n",
        "\n",
        "    # shift features across old and new metadata.\n",
        "    meta_selected = meta.select([\n",
        "        'patient_id',\n",
        "        'anatom_site_general',\n",
        "        'age_approx',\n",
        "        pl.lit(0).cast(pl.Int64).alias(\"binary_target\"),\n",
        "    ])\n",
        "    past_meta_selected = past_meta.select([\n",
        "        'patient_id',\n",
        "        'anatom_site_general',\n",
        "        'age_approx',\n",
        "        pl.col('binary_target')\n",
        "    ])\n",
        "    past_meta_selected = past_meta_selected.with_columns([\n",
        "        pl.col('anatom_site_general').cast(pl.Categorical),\n",
        "        pl.col('age_approx').cast(pl.Float64),\n",
        "        pl.col('binary_target').cast(pl.Int64),\n",
        "    ])\n",
        "    combined_df = pl.concat([meta_selected, past_meta_selected])\n",
        "    combined_df = combined_df.with_columns([\n",
        "        pl.col('age_approx').cast(pl.Float64)\n",
        "    ])\n",
        "    del meta_selected, past_meta_selected\n",
        "    gc.collect()\n",
        "\n",
        "    target_columns = ['binary_target']\n",
        "    aggregations=['mean']\n",
        "    agg_exprs = [\n",
        "        getattr(pl.col(col), agg)().alias(f'{col}_{agg}')\n",
        "        for col in target_columns\n",
        "        for agg in aggregations\n",
        "    ]\n",
        "\n",
        "    shift_features = []\n",
        "    for agg_cols in [\n",
        "        ['patient_id', 'anatom_site_general'],\n",
        "        ['patient_id'],\n",
        "    ]:\n",
        "        agg_df = combined_df.group_by(agg_cols + ['age_approx'], maintain_order=True).agg(agg_exprs)\n",
        "        for shift in range(1, shift_range + 1):\n",
        "            shift_exprs = []\n",
        "            diff_exprs = []\n",
        "\n",
        "            for col in target_columns:\n",
        "                for agg in aggregations:\n",
        "                    base_col = f'{col}_{agg}'\n",
        "\n",
        "                    # In real world we shouldn't use 'next' shift features.\n",
        "                    prev_col = f'{base_col}_prev_{shift}'\n",
        "                    next_col = f'{base_col}_next_{shift}'\n",
        "\n",
        "                    shift_exprs.extend([\n",
        "                        pl.col(base_col).shift(shift).over(agg_cols).alias(prev_col),\n",
        "                        pl.col(base_col).shift(-shift).over(agg_cols).alias(next_col)\n",
        "                    ])\n",
        "\n",
        "                    diff_exprs.extend([\n",
        "                        (pl.col(base_col) - pl.col(prev_col)).alias(f'{base_col}_diff_prev_{shift}'),\n",
        "                        (pl.col(base_col) - pl.col(next_col)).alias(f'{base_col}_diff_next_{shift}')\n",
        "                    ])\n",
        "\n",
        "                    shift_features.extend([prev_col, next_col, f'{base_col}_diff_prev_{shift}', f'{base_col}_diff_next_{shift}'])\n",
        "\n",
        "            agg_df = agg_df.with_columns(shift_exprs)\n",
        "            agg_df = agg_df.with_columns(diff_exprs)\n",
        "\n",
        "        meta = meta.join(agg_df, on=agg_cols + ['age_approx'], how='left')\n",
        "        del agg_df\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    join_keys = {'patient_id', 'age_approx', 'anatom_site_general'}\n",
        "    not_use_cols = {'benign', 'indeterminate', 'indeterminate/benign', 'indeterminate/malignant', 'malignant'}\n",
        "    all_new_feature_names = [\n",
        "        col for col in (\n",
        "            list(skin_df_cols[1:]) +\n",
        "            list(pivot_df_1_cols[1:]) +\n",
        "            list(pivot_df_2_cols[1:]) +\n",
        "            list(patient_aggregates_cols) +\n",
        "            list(patient_age_aggregates_cols) +\n",
        "            list(patient_age_site_aggregates_cols) +\n",
        "            new_feature_names +\n",
        "            shift_features\n",
        "        ) if col not in join_keys | not_use_cols\n",
        "    ]\n",
        "\n",
        "    return meta, all_new_feature_names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "mauVqNT-dA0b",
      "metadata": {
        "id": "mauVqNT-dA0b"
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, X, y, X_valid, y_valid, params, predictors):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.X_valid = X_valid\n",
        "        self.y_valid = y_valid\n",
        "        self.params = params\n",
        "        self.predictors = predictors\n",
        "\n",
        "    def fit(self):\n",
        "        oof_result = []\n",
        "        X_train, X_valid = self.X, self.X_valid\n",
        "        y_train, y_valid = self.y, self.y_valid\n",
        "\n",
        "        sample_weight_train = X_train['sample_weight']\n",
        "        sample_weight_valid = X_valid['sample_weight']\n",
        "\n",
        "        dtrain = lgb.Dataset(\n",
        "            X_train[self.predictors],\n",
        "            label=y_train,\n",
        "            feature_name=self.predictors,\n",
        "            weight=sample_weight_train,\n",
        "        )\n",
        "        dvalid = lgb.Dataset(\n",
        "            X_valid[self.predictors],\n",
        "            label=y_valid,\n",
        "            feature_name=self.predictors,\n",
        "            weight=sample_weight_valid,\n",
        "        )\n",
        "        del X_train, X_valid\n",
        "        gc.collect()\n",
        "\n",
        "        callbacks = [\n",
        "            lgb.log_evaluation(100),\n",
        "            lgb.early_stopping(200),\n",
        "        ]\n",
        "\n",
        "        clf = lgb.train(\n",
        "            self.params,\n",
        "            dtrain,\n",
        "            valid_sets=[dtrain, dvalid],\n",
        "            num_boost_round=100000,\n",
        "            categorical_feature=[],\n",
        "            callbacks=callbacks,\n",
        "        )\n",
        "        oof_result.append([x for x in clf.best_score[\"valid_1\"].values()][0])\n",
        "        self.clf = clf\n",
        "        self.oof_result = oof_result\n",
        "        return clf, oof_result\n",
        "\n",
        "    def fit_xgb(self):\n",
        "        oof_result = []\n",
        "        X_train, X_valid = self.X, self.X_valid\n",
        "        y_train, y_valid = self.y, self.y_valid\n",
        "\n",
        "        X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "        X_valid = X_valid.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_valid = X_valid.fillna(0)\n",
        "\n",
        "        sample_weight_train = X_train['sample_weight']\n",
        "        sample_weight_valid = X_valid['sample_weight']\n",
        "\n",
        "        X_train[self.predictors] = X_train[self.predictors].astype(np.float32)\n",
        "        X_valid[self.predictors] = X_valid[self.predictors].astype(np.float32)\n",
        "\n",
        "        dtrain = xgb.DMatrix(X_train[self.predictors], label=y_train, weight=sample_weight_train)\n",
        "        dvalid = xgb.DMatrix(X_valid[self.predictors], label=y_valid, weight=sample_weight_valid)\n",
        "        del X_train, X_valid\n",
        "        gc.collect()\n",
        "\n",
        "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
        "\n",
        "        clf = xgb.train(\n",
        "            self.params,\n",
        "            dtrain,\n",
        "            10000,\n",
        "            early_stopping_rounds=200,\n",
        "            evals=watchlist,\n",
        "            verbose_eval=50,\n",
        "        )\n",
        "\n",
        "        self.clf = clf\n",
        "        self.oof_result = oof_result\n",
        "        return clf, oof_result\n",
        "\n",
        "\n",
        "def read_img_oof(train_df):\n",
        "    oof_df_tsuma_eva_nes = pl.read_parquet(DATA_PATH / 'preds_eva_nes.parquet', columns=['isic_id', 'pred']).rename({'pred': 'pred_tsuma_eva_nes'})\n",
        "    oof_df_tsuma_conv_nes = pl.read_parquet(DATA_PATH / 'preds_conv_nes.parquet', columns=['isic_id', 'pred']).rename({'pred': 'pred_tsuma_conv_nes'})\n",
        "    oof_df_sub_71 = pl.concat([pl.read_csv(DATA_PATH / f'sub71/test_results_fold_{fold}.csv') for fold in range(5)]).select(['isic_id', 'pred']).rename({'pred': 'pred_sub_71'})\n",
        "    oof_df_sub_73 = pl.concat([pl.read_csv(DATA_PATH / f'sub73/test_results_fold_{fold}.csv') for fold in range(5)]).select(['isic_id', 'pred']).rename({'pred': 'pred_sub_73'})\n",
        "    oof_df_sub_75 = pl.concat([pl.read_csv(DATA_PATH / f'sub75/test_results_fold_{fold}.csv') for fold in range(5)]).select(['isic_id', 'pred']).rename({'pred': 'pred_sub_75'})\n",
        "    oof_df_sub_77 = pl.concat([pl.read_csv(DATA_PATH / f'sub77/test_results_fold_{fold}.csv') for fold in range(5)]).select(['isic_id', 'pred']).rename({'pred': 'pred_sub_77'})\n",
        "\n",
        "    # Merge the data\n",
        "    train_df = train_df.join(oof_df_tsuma_eva_nes, on='isic_id', how='left')\n",
        "    train_df = train_df.join(oof_df_tsuma_conv_nes, on='isic_id', how='left')\n",
        "    train_df = train_df.join(oof_df_sub_71, on='isic_id', how='left')\n",
        "    train_df = train_df.join(oof_df_sub_73, on='isic_id', how='left')\n",
        "    train_df = train_df.join(oof_df_sub_75, on='isic_id', how='left')\n",
        "    train_df = train_df.join(oof_df_sub_77, on='isic_id', how='left')\n",
        "\n",
        "    return train_df\n",
        "\n",
        "\n",
        "params_xgb = {\n",
        "    'objective':  'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    \"nthread\": -1,\n",
        "    \"learning_rate\" : 0.03,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'subsample': 0.6,\n",
        "    'max_depth': 7,\n",
        "    'lambda': 5,\n",
        "    'tree_method':\"hist\",\n",
        "    'scale_pos_weight':8,\n",
        "\n",
        "    'tree_method': 'hist',\n",
        "    'device': 'cuda',\n",
        "}\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"verbosity\": -1,\n",
        "    'learning_rate': 0.005,\n",
        "    'bagging_freq': 2,\n",
        "    'feature_fraction': 0.6,\n",
        "    'subsample': 0.6,\n",
        "    'lambda_l1': 1.5,\n",
        "    'lambda_l2': 5.5,\n",
        "    'num_leaves': 32,\n",
        "    \"min_data_in_leaf\": 20,\n",
        "    'scale_pos_weight': 5,\n",
        "    'device': 'gpu',  # use GPU\n",
        "    'max_bin': 127,\n",
        "    'gpu_platform_id': 0,\n",
        "    'gpu_device_id': 0,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "X9E9elTegHlF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9E9elTegHlF",
        "outputId": "5229bdfd-4ef3-4590-e295-f117c1c3769c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-0a9bf8c8d378>:372: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "  .agg(pl.count().alias(f'{col}_count'))\n",
            "<ipython-input-10-0a9bf8c8d378>:371: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
            "  df.group_by(['patient_id', col])\n",
            "<ipython-input-10-0a9bf8c8d378>:452: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
            "  pivot_df = group_count.pivot(\n",
            "<ipython-input-10-0a9bf8c8d378>:478: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "  meta = meta.join(pivot_df_1, on=['patient_id', 'anatom_site_general', 'age_approx'], how='left')\n",
            "<ipython-input-10-0a9bf8c8d378>:497: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "  meta = meta.join(pivot_df_2, on=['patient_id', 'anatom_site_general'], how='left')\n",
            "<ipython-input-10-0a9bf8c8d378>:556: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "  meta = meta.join(patient_age_aggregates, on=['patient_id', 'anatom_site_general'], how='left')\n",
            "<ipython-input-10-0a9bf8c8d378>:571: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "  meta = meta.join(patient_age_site_aggregates, on=['patient_id', 'anatom_site_general', 'age_approx'], how='left')\n",
            "<ipython-input-10-0a9bf8c8d378>:633: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "  past_meta_selected = past_meta_selected.with_columns([\n",
            "sys:1: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "<ipython-input-10-0a9bf8c8d378>:686: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
            "  meta = meta.join(agg_df, on=agg_cols + ['age_approx'], how='left')\n"
          ]
        }
      ],
      "source": [
        "train_df = read_data(TRAIN_PATH)\n",
        "train_df = feature_engineering_patient_id_pl(train_df, num_cols + new_num_cols)\n",
        "train_df = feature_engineering_age_pl(train_df)\n",
        "train_df = train_df.select([col for col in train_df.columns if not train_df[col].is_null().all()])\n",
        "\n",
        "past_meta = pl.read_csv(DATA_PATH / 'past_metadata.csv')\n",
        "train_df, new_feature_names = process_metadata(\n",
        "    past_meta,\n",
        "    train_df,\n",
        "    num_cols=num_cols+new_num_cols,\n",
        ")\n",
        "train_df = train_df.select([col for col in train_df.columns if not train_df[col].is_null().all()])\n",
        "train_df = read_img_oof(train_df)\n",
        "train_df = train_df.to_pandas()\n",
        "\n",
        "feature_cols += new_feature_names\n",
        "feature_cols = [\n",
        "    col for col in feature_cols\n",
        "    if col not in ['benign', 'indeterminate', 'indeterminate/benign', 'indeterminate/malignant', 'malignant']\n",
        "]\n",
        "feature_cols = sorted(list(set(feature_cols)))\n",
        "feature_cols = [col for col in feature_cols if col in train_df.columns]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aXhc_ZFCf8Ou",
      "metadata": {
        "id": "aXhc_ZFCf8Ou"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f1f4e838-ee0e-4f2e-a7ee-9bdd8eb3ddd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1f4e838-ee0e-4f2e-a7ee-9bdd8eb3ddd0",
        "outputId": "7eadf9ba-58ee-4625-f0ad-32ffbafb4dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exp_ver=1 img_cols=['pred_sub_71', 'pred_sub_73', 'pred_sub_75', 'pred_sub_77', 'pred_tsuma_eva', 'pred_tsuma_conv_nes'] attribution_flag=0 feature_select_flag=1 lesion_id_weight=0.0 Indeterminate_weight=0.0\n",
            "1834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12836\teval-logloss:0.12891\n",
            "[50]\ttrain-logloss:0.02990\teval-logloss:0.03204\n",
            "[100]\ttrain-logloss:0.00849\teval-logloss:0.01131\n",
            "[150]\ttrain-logloss:0.00322\teval-logloss:0.00656\n",
            "[200]\ttrain-logloss:0.00170\teval-logloss:0.00539\n",
            "[250]\ttrain-logloss:0.00112\teval-logloss:0.00510\n",
            "[300]\ttrain-logloss:0.00079\teval-logloss:0.00502\n",
            "[350]\ttrain-logloss:0.00058\teval-logloss:0.00505\n",
            "[400]\ttrain-logloss:0.00046\teval-logloss:0.00513\n",
            "[450]\ttrain-logloss:0.00037\teval-logloss:0.00522\n",
            "[500]\ttrain-logloss:0.00031\teval-logloss:0.00530\n",
            "[502]\ttrain-logloss:0.00030\teval-logloss:0.00530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12870\teval-logloss:0.12851\n",
            "[50]\ttrain-logloss:0.03012\teval-logloss:0.03142\n",
            "[100]\ttrain-logloss:0.00862\teval-logloss:0.01057\n",
            "[150]\ttrain-logloss:0.00333\teval-logloss:0.00576\n",
            "[200]\ttrain-logloss:0.00179\teval-logloss:0.00461\n",
            "[250]\ttrain-logloss:0.00120\teval-logloss:0.00431\n",
            "[300]\ttrain-logloss:0.00086\teval-logloss:0.00423\n",
            "[350]\ttrain-logloss:0.00064\teval-logloss:0.00425\n",
            "[400]\ttrain-logloss:0.00050\teval-logloss:0.00429\n",
            "[450]\ttrain-logloss:0.00041\teval-logloss:0.00436\n",
            "[500]\ttrain-logloss:0.00034\teval-logloss:0.00443\n",
            "[528]\ttrain-logloss:0.00031\teval-logloss:0.00447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12886\teval-logloss:0.12838\n",
            "[50]\ttrain-logloss:0.03037\teval-logloss:0.03111\n",
            "[100]\ttrain-logloss:0.00886\teval-logloss:0.01020\n",
            "[150]\ttrain-logloss:0.00353\teval-logloss:0.00523\n",
            "[200]\ttrain-logloss:0.00194\teval-logloss:0.00392\n",
            "[250]\ttrain-logloss:0.00129\teval-logloss:0.00351\n",
            "[300]\ttrain-logloss:0.00091\teval-logloss:0.00335\n",
            "[350]\ttrain-logloss:0.00067\teval-logloss:0.00331\n",
            "[400]\ttrain-logloss:0.00052\teval-logloss:0.00330\n",
            "[450]\ttrain-logloss:0.00042\teval-logloss:0.00334\n",
            "[500]\ttrain-logloss:0.00035\teval-logloss:0.00338\n",
            "[550]\ttrain-logloss:0.00029\teval-logloss:0.00343\n",
            "[572]\ttrain-logloss:0.00027\teval-logloss:0.00344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12842\teval-logloss:0.12871\n",
            "[50]\ttrain-logloss:0.02991\teval-logloss:0.03182\n",
            "[100]\ttrain-logloss:0.00844\teval-logloss:0.01114\n",
            "[150]\ttrain-logloss:0.00321\teval-logloss:0.00640\n",
            "[200]\ttrain-logloss:0.00169\teval-logloss:0.00523\n",
            "[250]\ttrain-logloss:0.00108\teval-logloss:0.00490\n",
            "[300]\ttrain-logloss:0.00078\teval-logloss:0.00485\n",
            "[350]\ttrain-logloss:0.00058\teval-logloss:0.00489\n",
            "[400]\ttrain-logloss:0.00045\teval-logloss:0.00496\n",
            "[450]\ttrain-logloss:0.00036\teval-logloss:0.00503\n",
            "[498]\ttrain-logloss:0.00030\teval-logloss:0.00512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12852\teval-logloss:0.12863\n",
            "[50]\ttrain-logloss:0.03016\teval-logloss:0.03122\n",
            "[100]\ttrain-logloss:0.00866\teval-logloss:0.01036\n",
            "[150]\ttrain-logloss:0.00341\teval-logloss:0.00555\n",
            "[200]\ttrain-logloss:0.00186\teval-logloss:0.00442\n",
            "[250]\ttrain-logloss:0.00122\teval-logloss:0.00414\n",
            "[300]\ttrain-logloss:0.00086\teval-logloss:0.00412\n",
            "[350]\ttrain-logloss:0.00064\teval-logloss:0.00418\n",
            "[400]\ttrain-logloss:0.00050\teval-logloss:0.00427\n",
            "[450]\ttrain-logloss:0.00040\teval-logloss:0.00436\n",
            "[479]\ttrain-logloss:0.00036\teval-logloss:0.00440\n",
            "0.1764551657764271\n",
            "0.18243941361029964\n",
            "0.1927567614014736\n",
            "0.1755059634002985\n",
            "0.19128666989086976\n",
            "0.18368879481587372\n",
            "0.182934145563667\n",
            "exp_ver=2 img_cols=['pred_sub_71', 'pred_sub_73', 'pred_sub_75', 'pred_sub_77', 'pred_tsuma_eva'] attribution_flag=0 feature_select_flag=1 lesion_id_weight=0.0 Indeterminate_weight=0.0\n",
            "1833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12836\teval-logloss:0.12891\n",
            "[50]\ttrain-logloss:0.02994\teval-logloss:0.03208\n",
            "[100]\ttrain-logloss:0.00847\teval-logloss:0.01130\n",
            "[150]\ttrain-logloss:0.00321\teval-logloss:0.00658\n",
            "[200]\ttrain-logloss:0.00169\teval-logloss:0.00537\n",
            "[250]\ttrain-logloss:0.00111\teval-logloss:0.00505\n",
            "[300]\ttrain-logloss:0.00080\teval-logloss:0.00495\n",
            "[350]\ttrain-logloss:0.00059\teval-logloss:0.00499\n",
            "[400]\ttrain-logloss:0.00046\teval-logloss:0.00504\n",
            "[450]\ttrain-logloss:0.00037\teval-logloss:0.00512\n",
            "[500]\ttrain-logloss:0.00031\teval-logloss:0.00520\n",
            "[504]\ttrain-logloss:0.00030\teval-logloss:0.00520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12870\teval-logloss:0.12850\n",
            "[50]\ttrain-logloss:0.03017\teval-logloss:0.03136\n",
            "[100]\ttrain-logloss:0.00865\teval-logloss:0.01053\n",
            "[150]\ttrain-logloss:0.00335\teval-logloss:0.00574\n",
            "[200]\ttrain-logloss:0.00179\teval-logloss:0.00460\n",
            "[250]\ttrain-logloss:0.00118\teval-logloss:0.00431\n",
            "[300]\ttrain-logloss:0.00084\teval-logloss:0.00426\n",
            "[350]\ttrain-logloss:0.00063\teval-logloss:0.00428\n",
            "[400]\ttrain-logloss:0.00050\teval-logloss:0.00433\n",
            "[450]\ttrain-logloss:0.00040\teval-logloss:0.00441\n",
            "[500]\ttrain-logloss:0.00033\teval-logloss:0.00450\n",
            "[522]\ttrain-logloss:0.00031\teval-logloss:0.00454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12888\teval-logloss:0.12841\n",
            "[50]\ttrain-logloss:0.03043\teval-logloss:0.03117\n",
            "[100]\ttrain-logloss:0.00887\teval-logloss:0.01022\n",
            "[150]\ttrain-logloss:0.00351\teval-logloss:0.00523\n",
            "[200]\ttrain-logloss:0.00192\teval-logloss:0.00390\n",
            "[250]\ttrain-logloss:0.00128\teval-logloss:0.00350\n",
            "[300]\ttrain-logloss:0.00090\teval-logloss:0.00334\n",
            "[350]\ttrain-logloss:0.00068\teval-logloss:0.00329\n",
            "[400]\ttrain-logloss:0.00053\teval-logloss:0.00328\n",
            "[450]\ttrain-logloss:0.00043\teval-logloss:0.00331\n",
            "[500]\ttrain-logloss:0.00035\teval-logloss:0.00333\n",
            "[550]\ttrain-logloss:0.00029\teval-logloss:0.00337\n",
            "[560]\ttrain-logloss:0.00028\teval-logloss:0.00338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12844\teval-logloss:0.12872\n",
            "[50]\ttrain-logloss:0.02992\teval-logloss:0.03180\n",
            "[100]\ttrain-logloss:0.00843\teval-logloss:0.01106\n",
            "[150]\ttrain-logloss:0.00318\teval-logloss:0.00634\n",
            "[200]\ttrain-logloss:0.00166\teval-logloss:0.00521\n",
            "[250]\ttrain-logloss:0.00107\teval-logloss:0.00490\n",
            "[300]\ttrain-logloss:0.00076\teval-logloss:0.00485\n",
            "[350]\ttrain-logloss:0.00057\teval-logloss:0.00490\n",
            "[400]\ttrain-logloss:0.00045\teval-logloss:0.00496\n",
            "[450]\ttrain-logloss:0.00036\teval-logloss:0.00504\n",
            "[485]\ttrain-logloss:0.00032\teval-logloss:0.00510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-071c488c49ca>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-13-071c488c49ca>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.12852\teval-logloss:0.12863\n",
            "[50]\ttrain-logloss:0.03016\teval-logloss:0.03119\n",
            "[100]\ttrain-logloss:0.00868\teval-logloss:0.01035\n",
            "[150]\ttrain-logloss:0.00339\teval-logloss:0.00553\n",
            "[200]\ttrain-logloss:0.00183\teval-logloss:0.00437\n",
            "[250]\ttrain-logloss:0.00122\teval-logloss:0.00409\n",
            "[300]\ttrain-logloss:0.00086\teval-logloss:0.00407\n",
            "[350]\ttrain-logloss:0.00064\teval-logloss:0.00413\n",
            "[400]\ttrain-logloss:0.00050\teval-logloss:0.00422\n",
            "[450]\ttrain-logloss:0.00040\teval-logloss:0.00430\n",
            "[479]\ttrain-logloss:0.00036\teval-logloss:0.00436\n",
            "0.17716424991478408\n",
            "0.1843440276778196\n",
            "0.19276664303666943\n",
            "0.1736660474243875\n",
            "0.1906764855082317\n",
            "0.18372349071237845\n",
            "0.18299619682846377\n"
          ]
        }
      ],
      "source": [
        "le_dict = {}\n",
        "for c in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(pd.concat([train_df[c]]))\n",
        "    train_df[c] = le.transform(train_df[c])\n",
        "    le.classes_ = np.append(le.classes_, '<unknown>')\n",
        "    le_dict[c] = le\n",
        "\n",
        "\n",
        "for CFG in all_config_list:\n",
        "    print(CFG)\n",
        "    feature_cols_with_img = feature_cols + CFG.img_cols\n",
        "\n",
        "    if CFG.attribution_flag:\n",
        "        feature_cols_with_img = [col for col in feature_cols_with_img if 'attribution' not in col]\n",
        "\n",
        "    if CFG.feature_select_flag:\n",
        "        imp_df = pd.read_csv('/content/drive/MyDrive/kaggle/isic2024/lgb_importance.csv')\n",
        "        imp_df_gp = imp_df.groupby('feature_name')[['imporatance_gain']].mean().reset_index()\n",
        "        drop_features = imp_df_gp[imp_df_gp.imporatance_gain == 0].feature_name.values\n",
        "        feature_cols_with_img = [col for col in feature_cols_with_img if col not in drop_features]\n",
        "        print(len(feature_cols_with_img))\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR / f\"exp{CFG.exp_ver}\", exist_ok=True)\n",
        "    joblib.dump(le_dict, OUTPUT_DIR / f\"exp{CFG.exp_ver}\" / \"labelEncoder.joblib\")\n",
        "\n",
        "    gkf = StratifiedGroupKFold(n_splits=N_SPLITS, random_state=SPLIT_SEED, shuffle=True)\n",
        "    train_df[\"fold\"] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(\n",
        "        gkf.split(train_df, train_df['target'], groups=train_df[\"patient_id\"])\n",
        "    ):\n",
        "        train_df.loc[val_idx, \"fold\"] = fold\n",
        "\n",
        "    xgb_clfs = []\n",
        "    for fold in range(5):\n",
        "        X = train_df[train_df[\"fold\"] != fold]\n",
        "        y = X[TARGET_COL].astype(float)\n",
        "\n",
        "        X_valid = train_df[train_df[\"fold\"] == fold]\n",
        "        y_valid = X_valid[TARGET_COL].astype(float)\n",
        "\n",
        "        X['sample_weight'] = 1.0\n",
        "        X_valid['sample_weight'] = 1.0\n",
        "\n",
        "        if CFG.lesion_id_weight > 0:\n",
        "            X.loc[X['lesion_id'].notnull() & (X['target'] == 0), 'sample_weight'] = 0.1\n",
        "            X.loc[X['lesion_id'].notnull(), 'target'] = 1\n",
        "            X_valid.loc[X_valid['lesion_id'].notnull() & (X_valid['target'] == 0), 'sample_weight'] = 0.1\n",
        "            X_valid.loc[X_valid['lesion_id'].notnull(), 'target'] = 1\n",
        "\n",
        "        if CFG.Indeterminate_weight > 0:\n",
        "            X.loc[X['iddx_1'] == 'Indeterminate', 'target'] = 1\n",
        "            X.loc[X['iddx_1'] == 'Indeterminate', 'sample_weight'] = 0.5\n",
        "            X_valid.loc[X_valid['iddx_1'] == 'Indeterminate', 'target'] = 1\n",
        "            X_valid.loc[X_valid['iddx_1'] == 'Indeterminate', 'sample_weight'] = 0.5\n",
        "\n",
        "        trainer = Trainer(X, y, X_valid, y_valid, params_xgb, feature_cols_with_img)\n",
        "        trainer.fit_xgb()\n",
        "        xgb_clfs.append(trainer.clf)\n",
        "        trainer.clf.save_model(OUTPUT_DIR / f'exp{CFG.exp_ver}' / f\"xgb_fold_{fold}.json\")\n",
        "\n",
        "    xgb_oof_df = train_df[['isic_id', TARGET_COL]].copy()\n",
        "    xgb_oof_scores = []\n",
        "    for fold in range(5):\n",
        "        X = train_df[train_df[\"fold\"] != fold]\n",
        "        X_valid = train_df[train_df[\"fold\"] == fold].copy()\n",
        "\n",
        "        X_valid = X_valid.replace([np.inf, -np.inf], np.nan)\n",
        "        X_valid = X_valid.fillna(0)\n",
        "\n",
        "        dvalid = xgb.DMatrix(X_valid[feature_cols_with_img])\n",
        "\n",
        "        tmp_xgb_clf = xgb.Booster()\n",
        "        tmp_xgb_clf.load_model(OUTPUT_DIR / f'exp{CFG.exp_ver}' / f\"xgb_fold_{fold}.json\")\n",
        "\n",
        "        tmp = tmp_xgb_clf.predict(dvalid)\n",
        "        print(score(X_valid[TARGET_COL], tmp))\n",
        "        xgb_oof_scores.append(score(X_valid[TARGET_COL], tmp))\n",
        "\n",
        "        xgb_oof_df.loc[X_valid.index, 'pred'] = tmp\n",
        "\n",
        "    print(np.mean(xgb_oof_scores))\n",
        "    param_sheet.update_acell(f\"H{CFG.exp_ver+1}\", np.mean(xgb_oof_scores))\n",
        "    print(score(xgb_oof_df[TARGET_COL], xgb_oof_df['pred']))\n",
        "    param_sheet.update_acell(f\"I{CFG.exp_ver+1}\", score(xgb_oof_df[TARGET_COL], xgb_oof_df['pred']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "00146fdc-a32c-4106-ad57-e43aedba46e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00146fdc-a32c-4106-ad57-e43aedba46e1",
        "outputId": "d506e28e-1605-40b2-8fa6-04266b70d69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exp_ver=1 img_cols=['pred_sub_71', 'pred_sub_73', 'pred_sub_75', 'pred_sub_77', 'pred_tsuma_eva', 'pred_tsuma_conv_nes'] attribution_flag=0 feature_select_flag=1 lesion_id_weight=0.0 Indeterminate_weight=0.0\n",
            "1834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.0036065\tvalid_1's binary_logloss: 0.00609867\n",
            "[200]\ttraining's binary_logloss: 0.00292092\tvalid_1's binary_logloss: 0.00589104\n",
            "[300]\ttraining's binary_logloss: 0.0025067\tvalid_1's binary_logloss: 0.00578578\n",
            "[400]\ttraining's binary_logloss: 0.00221231\tvalid_1's binary_logloss: 0.00570062\n",
            "[500]\ttraining's binary_logloss: 0.00197092\tvalid_1's binary_logloss: 0.00561969\n",
            "[600]\ttraining's binary_logloss: 0.00176492\tvalid_1's binary_logloss: 0.00554408\n",
            "[700]\ttraining's binary_logloss: 0.00158276\tvalid_1's binary_logloss: 0.00546754\n",
            "[800]\ttraining's binary_logloss: 0.00143268\tvalid_1's binary_logloss: 0.00541131\n",
            "[900]\ttraining's binary_logloss: 0.00129956\tvalid_1's binary_logloss: 0.00536958\n",
            "[1000]\ttraining's binary_logloss: 0.00118133\tvalid_1's binary_logloss: 0.00533133\n",
            "[1100]\ttraining's binary_logloss: 0.00107458\tvalid_1's binary_logloss: 0.00530128\n",
            "[1200]\ttraining's binary_logloss: 0.000984217\tvalid_1's binary_logloss: 0.00527676\n",
            "[1300]\ttraining's binary_logloss: 0.000903265\tvalid_1's binary_logloss: 0.00526484\n",
            "[1400]\ttraining's binary_logloss: 0.000832997\tvalid_1's binary_logloss: 0.0052689\n",
            "[1500]\ttraining's binary_logloss: 0.000767181\tvalid_1's binary_logloss: 0.00526768\n",
            "Early stopping, best iteration is:\n",
            "[1321]\ttraining's binary_logloss: 0.000887584\tvalid_1's binary_logloss: 0.00526182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00377259\tvalid_1's binary_logloss: 0.00511677\n",
            "[200]\ttraining's binary_logloss: 0.00304029\tvalid_1's binary_logloss: 0.00484684\n",
            "[300]\ttraining's binary_logloss: 0.00261039\tvalid_1's binary_logloss: 0.00471026\n",
            "[400]\ttraining's binary_logloss: 0.00230591\tvalid_1's binary_logloss: 0.00462485\n",
            "[500]\ttraining's binary_logloss: 0.00204966\tvalid_1's binary_logloss: 0.00455322\n",
            "[600]\ttraining's binary_logloss: 0.00184075\tvalid_1's binary_logloss: 0.00450175\n",
            "[700]\ttraining's binary_logloss: 0.00166071\tvalid_1's binary_logloss: 0.00446687\n",
            "[800]\ttraining's binary_logloss: 0.0015049\tvalid_1's binary_logloss: 0.00443138\n",
            "[900]\ttraining's binary_logloss: 0.00136619\tvalid_1's binary_logloss: 0.00440509\n",
            "[1000]\ttraining's binary_logloss: 0.00124762\tvalid_1's binary_logloss: 0.00438777\n",
            "[1100]\ttraining's binary_logloss: 0.00114214\tvalid_1's binary_logloss: 0.00437442\n",
            "[1200]\ttraining's binary_logloss: 0.00104298\tvalid_1's binary_logloss: 0.00436881\n",
            "[1300]\ttraining's binary_logloss: 0.000956814\tvalid_1's binary_logloss: 0.0043705\n",
            "Early stopping, best iteration is:\n",
            "[1197]\ttraining's binary_logloss: 0.00104603\tvalid_1's binary_logloss: 0.00436766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00394473\tvalid_1's binary_logloss: 0.00458286\n",
            "[200]\ttraining's binary_logloss: 0.00322949\tvalid_1's binary_logloss: 0.00448626\n",
            "[300]\ttraining's binary_logloss: 0.00281233\tvalid_1's binary_logloss: 0.00441639\n",
            "[400]\ttraining's binary_logloss: 0.00249865\tvalid_1's binary_logloss: 0.00434211\n",
            "[500]\ttraining's binary_logloss: 0.0022394\tvalid_1's binary_logloss: 0.00425442\n",
            "[600]\ttraining's binary_logloss: 0.00201641\tvalid_1's binary_logloss: 0.00417757\n",
            "[700]\ttraining's binary_logloss: 0.00181926\tvalid_1's binary_logloss: 0.00409166\n",
            "[800]\ttraining's binary_logloss: 0.00164185\tvalid_1's binary_logloss: 0.00401008\n",
            "[900]\ttraining's binary_logloss: 0.00149117\tvalid_1's binary_logloss: 0.00395098\n",
            "[1000]\ttraining's binary_logloss: 0.0013633\tvalid_1's binary_logloss: 0.00390037\n",
            "[1100]\ttraining's binary_logloss: 0.00124786\tvalid_1's binary_logloss: 0.00385546\n",
            "[1200]\ttraining's binary_logloss: 0.00114081\tvalid_1's binary_logloss: 0.00381612\n",
            "[1300]\ttraining's binary_logloss: 0.00105293\tvalid_1's binary_logloss: 0.00379197\n",
            "[1400]\ttraining's binary_logloss: 0.000972034\tvalid_1's binary_logloss: 0.00375983\n",
            "[1500]\ttraining's binary_logloss: 0.000901811\tvalid_1's binary_logloss: 0.00374162\n",
            "[1600]\ttraining's binary_logloss: 0.000836653\tvalid_1's binary_logloss: 0.00372763\n",
            "[1700]\ttraining's binary_logloss: 0.000779487\tvalid_1's binary_logloss: 0.0037153\n",
            "[1800]\ttraining's binary_logloss: 0.000727568\tvalid_1's binary_logloss: 0.00370379\n",
            "[1900]\ttraining's binary_logloss: 0.000681323\tvalid_1's binary_logloss: 0.00369799\n",
            "[2000]\ttraining's binary_logloss: 0.000637403\tvalid_1's binary_logloss: 0.00369459\n",
            "[2100]\ttraining's binary_logloss: 0.00059933\tvalid_1's binary_logloss: 0.00369018\n",
            "[2200]\ttraining's binary_logloss: 0.000565323\tvalid_1's binary_logloss: 0.00369039\n",
            "[2300]\ttraining's binary_logloss: 0.000532576\tvalid_1's binary_logloss: 0.00369289\n",
            "[2400]\ttraining's binary_logloss: 0.000504319\tvalid_1's binary_logloss: 0.00369259\n",
            "Early stopping, best iteration is:\n",
            "[2235]\ttraining's binary_logloss: 0.00055396\tvalid_1's binary_logloss: 0.00368871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00360648\tvalid_1's binary_logloss: 0.00604384\n",
            "[200]\ttraining's binary_logloss: 0.00290663\tvalid_1's binary_logloss: 0.00572621\n",
            "[300]\ttraining's binary_logloss: 0.0024991\tvalid_1's binary_logloss: 0.00559161\n",
            "[400]\ttraining's binary_logloss: 0.00220482\tvalid_1's binary_logloss: 0.00549287\n",
            "[500]\ttraining's binary_logloss: 0.00196358\tvalid_1's binary_logloss: 0.005419\n",
            "[600]\ttraining's binary_logloss: 0.00176081\tvalid_1's binary_logloss: 0.00535009\n",
            "[700]\ttraining's binary_logloss: 0.00158371\tvalid_1's binary_logloss: 0.00529768\n",
            "[800]\ttraining's binary_logloss: 0.00143163\tvalid_1's binary_logloss: 0.00525463\n",
            "[900]\ttraining's binary_logloss: 0.00129738\tvalid_1's binary_logloss: 0.00522305\n",
            "[1000]\ttraining's binary_logloss: 0.00118073\tvalid_1's binary_logloss: 0.00520458\n",
            "[1100]\ttraining's binary_logloss: 0.00107778\tvalid_1's binary_logloss: 0.00518852\n",
            "[1200]\ttraining's binary_logloss: 0.000988354\tvalid_1's binary_logloss: 0.00517743\n",
            "[1300]\ttraining's binary_logloss: 0.000903692\tvalid_1's binary_logloss: 0.00517652\n",
            "[1400]\ttraining's binary_logloss: 0.000833432\tvalid_1's binary_logloss: 0.00517591\n",
            "[1500]\ttraining's binary_logloss: 0.000767844\tvalid_1's binary_logloss: 0.00517332\n",
            "[1600]\ttraining's binary_logloss: 0.000713414\tvalid_1's binary_logloss: 0.00518314\n",
            "Early stopping, best iteration is:\n",
            "[1484]\ttraining's binary_logloss: 0.000777961\tvalid_1's binary_logloss: 0.00517164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00376805\tvalid_1's binary_logloss: 0.00543226\n",
            "[200]\ttraining's binary_logloss: 0.00306777\tvalid_1's binary_logloss: 0.0050006\n",
            "[300]\ttraining's binary_logloss: 0.00264911\tvalid_1's binary_logloss: 0.00479254\n",
            "[400]\ttraining's binary_logloss: 0.00234001\tvalid_1's binary_logloss: 0.00465063\n",
            "[500]\ttraining's binary_logloss: 0.00209116\tvalid_1's binary_logloss: 0.00454752\n",
            "[600]\ttraining's binary_logloss: 0.00187743\tvalid_1's binary_logloss: 0.0044795\n",
            "[700]\ttraining's binary_logloss: 0.00169569\tvalid_1's binary_logloss: 0.00442366\n",
            "[800]\ttraining's binary_logloss: 0.00153149\tvalid_1's binary_logloss: 0.00438556\n",
            "[900]\ttraining's binary_logloss: 0.00138818\tvalid_1's binary_logloss: 0.00436105\n",
            "[1000]\ttraining's binary_logloss: 0.00126259\tvalid_1's binary_logloss: 0.00434436\n",
            "[1100]\ttraining's binary_logloss: 0.00115288\tvalid_1's binary_logloss: 0.00434191\n",
            "[1200]\ttraining's binary_logloss: 0.00105791\tvalid_1's binary_logloss: 0.00433919\n",
            "[1300]\ttraining's binary_logloss: 0.000969348\tvalid_1's binary_logloss: 0.00434919\n",
            "Early stopping, best iteration is:\n",
            "[1144]\ttraining's binary_logloss: 0.00111073\tvalid_1's binary_logloss: 0.00433802\n",
            "0.17717008069700896\n",
            "0.18406111675580575\n",
            "0.18965564649524172\n",
            "0.17113215551364905\n",
            "0.1895848582276894\n",
            "0.18232077153787896\n",
            "0.18102505765559374\n",
            "exp_ver=2 img_cols=['pred_sub_71', 'pred_sub_73', 'pred_sub_75', 'pred_sub_77', 'pred_tsuma_eva'] attribution_flag=0 feature_select_flag=1 lesion_id_weight=0.0 Indeterminate_weight=0.0\n",
            "1833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00358963\tvalid_1's binary_logloss: 0.00606453\n",
            "[200]\ttraining's binary_logloss: 0.00289368\tvalid_1's binary_logloss: 0.00583597\n",
            "[300]\ttraining's binary_logloss: 0.00248622\tvalid_1's binary_logloss: 0.00573338\n",
            "[400]\ttraining's binary_logloss: 0.0021967\tvalid_1's binary_logloss: 0.00566628\n",
            "[500]\ttraining's binary_logloss: 0.00196136\tvalid_1's binary_logloss: 0.00559075\n",
            "[600]\ttraining's binary_logloss: 0.0017555\tvalid_1's binary_logloss: 0.0055094\n",
            "[700]\ttraining's binary_logloss: 0.00157594\tvalid_1's binary_logloss: 0.00544191\n",
            "[800]\ttraining's binary_logloss: 0.001427\tvalid_1's binary_logloss: 0.00538771\n",
            "[900]\ttraining's binary_logloss: 0.00129566\tvalid_1's binary_logloss: 0.00534407\n",
            "[1000]\ttraining's binary_logloss: 0.00118001\tvalid_1's binary_logloss: 0.0053107\n",
            "[1100]\ttraining's binary_logloss: 0.00107272\tvalid_1's binary_logloss: 0.00527972\n",
            "[1200]\ttraining's binary_logloss: 0.000980897\tvalid_1's binary_logloss: 0.00525699\n",
            "[1300]\ttraining's binary_logloss: 0.000901259\tvalid_1's binary_logloss: 0.0052482\n",
            "[1400]\ttraining's binary_logloss: 0.000830746\tvalid_1's binary_logloss: 0.0052489\n",
            "[1500]\ttraining's binary_logloss: 0.000766667\tvalid_1's binary_logloss: 0.00524649\n",
            "Early stopping, best iteration is:\n",
            "[1360]\ttraining's binary_logloss: 0.000858964\tvalid_1's binary_logloss: 0.00524421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00375692\tvalid_1's binary_logloss: 0.00512309\n",
            "[200]\ttraining's binary_logloss: 0.00302724\tvalid_1's binary_logloss: 0.00482814\n",
            "[300]\ttraining's binary_logloss: 0.00261094\tvalid_1's binary_logloss: 0.00468305\n",
            "[400]\ttraining's binary_logloss: 0.00231047\tvalid_1's binary_logloss: 0.00460275\n",
            "[500]\ttraining's binary_logloss: 0.002059\tvalid_1's binary_logloss: 0.00453035\n",
            "[600]\ttraining's binary_logloss: 0.00185473\tvalid_1's binary_logloss: 0.00448216\n",
            "[700]\ttraining's binary_logloss: 0.00167484\tvalid_1's binary_logloss: 0.00444018\n",
            "[800]\ttraining's binary_logloss: 0.00151773\tvalid_1's binary_logloss: 0.0044148\n",
            "[900]\ttraining's binary_logloss: 0.0013788\tvalid_1's binary_logloss: 0.00439788\n",
            "[1000]\ttraining's binary_logloss: 0.00125889\tvalid_1's binary_logloss: 0.00438689\n",
            "[1100]\ttraining's binary_logloss: 0.0011566\tvalid_1's binary_logloss: 0.00437905\n",
            "[1200]\ttraining's binary_logloss: 0.0010577\tvalid_1's binary_logloss: 0.00438168\n",
            "[1300]\ttraining's binary_logloss: 0.000970587\tvalid_1's binary_logloss: 0.00438616\n",
            "Early stopping, best iteration is:\n",
            "[1121]\ttraining's binary_logloss: 0.0011332\tvalid_1's binary_logloss: 0.00437715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00393941\tvalid_1's binary_logloss: 0.00452808\n",
            "[200]\ttraining's binary_logloss: 0.00322241\tvalid_1's binary_logloss: 0.0044231\n",
            "[300]\ttraining's binary_logloss: 0.00280622\tvalid_1's binary_logloss: 0.00434212\n",
            "[400]\ttraining's binary_logloss: 0.00249398\tvalid_1's binary_logloss: 0.00426205\n",
            "[500]\ttraining's binary_logloss: 0.00223589\tvalid_1's binary_logloss: 0.00418742\n",
            "[600]\ttraining's binary_logloss: 0.00201353\tvalid_1's binary_logloss: 0.00410992\n",
            "[700]\ttraining's binary_logloss: 0.00182152\tvalid_1's binary_logloss: 0.00404117\n",
            "[800]\ttraining's binary_logloss: 0.00164714\tvalid_1's binary_logloss: 0.00396966\n",
            "[900]\ttraining's binary_logloss: 0.00149538\tvalid_1's binary_logloss: 0.00390856\n",
            "[1000]\ttraining's binary_logloss: 0.00136542\tvalid_1's binary_logloss: 0.00385355\n",
            "[1100]\ttraining's binary_logloss: 0.00124632\tvalid_1's binary_logloss: 0.00380158\n",
            "[1200]\ttraining's binary_logloss: 0.00114156\tvalid_1's binary_logloss: 0.00376017\n",
            "[1300]\ttraining's binary_logloss: 0.0010531\tvalid_1's binary_logloss: 0.00372901\n",
            "[1400]\ttraining's binary_logloss: 0.000972344\tvalid_1's binary_logloss: 0.00369654\n",
            "[1500]\ttraining's binary_logloss: 0.000901608\tvalid_1's binary_logloss: 0.00367134\n",
            "[1600]\ttraining's binary_logloss: 0.000837121\tvalid_1's binary_logloss: 0.00365651\n",
            "[1700]\ttraining's binary_logloss: 0.000780542\tvalid_1's binary_logloss: 0.00364293\n",
            "[1800]\ttraining's binary_logloss: 0.00072957\tvalid_1's binary_logloss: 0.00363325\n",
            "[1900]\ttraining's binary_logloss: 0.000682198\tvalid_1's binary_logloss: 0.00362593\n",
            "[2000]\ttraining's binary_logloss: 0.000639585\tvalid_1's binary_logloss: 0.00361783\n",
            "[2100]\ttraining's binary_logloss: 0.000601449\tvalid_1's binary_logloss: 0.00361159\n",
            "[2200]\ttraining's binary_logloss: 0.000567818\tvalid_1's binary_logloss: 0.00361092\n",
            "[2300]\ttraining's binary_logloss: 0.000535375\tvalid_1's binary_logloss: 0.00361207\n",
            "Early stopping, best iteration is:\n",
            "[2160]\ttraining's binary_logloss: 0.000580735\tvalid_1's binary_logloss: 0.00360952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00360558\tvalid_1's binary_logloss: 0.00600712\n",
            "[200]\ttraining's binary_logloss: 0.00292229\tvalid_1's binary_logloss: 0.00569643\n",
            "[300]\ttraining's binary_logloss: 0.00252496\tvalid_1's binary_logloss: 0.00554714\n",
            "[400]\ttraining's binary_logloss: 0.00222599\tvalid_1's binary_logloss: 0.00543679\n",
            "[500]\ttraining's binary_logloss: 0.00198509\tvalid_1's binary_logloss: 0.005356\n",
            "[600]\ttraining's binary_logloss: 0.00177776\tvalid_1's binary_logloss: 0.00529212\n",
            "[700]\ttraining's binary_logloss: 0.00160033\tvalid_1's binary_logloss: 0.00523313\n",
            "[800]\ttraining's binary_logloss: 0.00144519\tvalid_1's binary_logloss: 0.00518945\n",
            "[900]\ttraining's binary_logloss: 0.00130876\tvalid_1's binary_logloss: 0.00515652\n",
            "[1000]\ttraining's binary_logloss: 0.00119276\tvalid_1's binary_logloss: 0.00513357\n",
            "[1100]\ttraining's binary_logloss: 0.00108919\tvalid_1's binary_logloss: 0.00512324\n",
            "[1200]\ttraining's binary_logloss: 0.000998593\tvalid_1's binary_logloss: 0.00511035\n",
            "[1300]\ttraining's binary_logloss: 0.000913955\tvalid_1's binary_logloss: 0.0051061\n",
            "[1400]\ttraining's binary_logloss: 0.000842955\tvalid_1's binary_logloss: 0.00510441\n",
            "[1500]\ttraining's binary_logloss: 0.000778815\tvalid_1's binary_logloss: 0.00510731\n",
            "Early stopping, best iteration is:\n",
            "[1353]\ttraining's binary_logloss: 0.000874736\tvalid_1's binary_logloss: 0.00510197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-61245518447b>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['sample_weight'] = 1.0\n",
            "<ipython-input-16-61245518447b>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_valid['sample_weight'] = 1.0\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:186: LGBMDeprecationWarning: Argument 'categorical_feature' to train() is deprecated and will be removed in a future release. Set 'categorical_feature' when calling lightgbm.Dataset() instead. See https://github.com/microsoft/LightGBM/issues/6435.\n",
            "  _emit_dataset_kwarg_warning(\"train\", \"categorical_feature\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n",
            "[100]\ttraining's binary_logloss: 0.00376371\tvalid_1's binary_logloss: 0.00541127\n",
            "[200]\ttraining's binary_logloss: 0.00308323\tvalid_1's binary_logloss: 0.00500658\n",
            "[300]\ttraining's binary_logloss: 0.002667\tvalid_1's binary_logloss: 0.00478941\n",
            "[400]\ttraining's binary_logloss: 0.00235789\tvalid_1's binary_logloss: 0.00465151\n",
            "[500]\ttraining's binary_logloss: 0.00210875\tvalid_1's binary_logloss: 0.00453855\n",
            "[600]\ttraining's binary_logloss: 0.00189185\tvalid_1's binary_logloss: 0.00445748\n",
            "[700]\ttraining's binary_logloss: 0.00170612\tvalid_1's binary_logloss: 0.00440258\n",
            "[800]\ttraining's binary_logloss: 0.00153979\tvalid_1's binary_logloss: 0.00436614\n",
            "[900]\ttraining's binary_logloss: 0.00139689\tvalid_1's binary_logloss: 0.00434174\n",
            "[1000]\ttraining's binary_logloss: 0.00126911\tvalid_1's binary_logloss: 0.00432719\n",
            "[1100]\ttraining's binary_logloss: 0.00116109\tvalid_1's binary_logloss: 0.0043301\n",
            "[1200]\ttraining's binary_logloss: 0.00106487\tvalid_1's binary_logloss: 0.00432913\n",
            "Early stopping, best iteration is:\n",
            "[1014]\ttraining's binary_logloss: 0.00125309\tvalid_1's binary_logloss: 0.00432402\n",
            "0.17646570864428715\n",
            "0.18372799157058922\n",
            "0.19082683508410128\n",
            "0.17147506857181713\n",
            "0.18968885651547202\n",
            "0.18243689207725336\n",
            "0.18123420306716032\n"
          ]
        }
      ],
      "source": [
        "le_dict = {}\n",
        "for c in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(pd.concat([train_df[c]]))\n",
        "    train_df[c] = le.transform(train_df[c])\n",
        "    le.classes_ = np.append(le.classes_, '<unknown>')\n",
        "    le_dict[c] = le\n",
        "\n",
        "for CFG in all_config_list:\n",
        "    print(CFG)\n",
        "    feature_cols_with_img = feature_cols + CFG.img_cols\n",
        "    # 分岐追加：attribution_flagが有効な場合、特定の列を除外\n",
        "    if CFG.attribution_flag:\n",
        "        feature_cols_with_img = [col for col in feature_cols_with_img if 'attribution' not in col]\n",
        "\n",
        "    if CFG.feature_select_flag:\n",
        "        imp_df = pd.read_csv('/content/drive/MyDrive/kaggle/isic2024/lgb_importance.csv')\n",
        "        imp_df_gp = imp_df.groupby('feature_name')[['imporatance_gain']].mean().reset_index()\n",
        "        drop_features = imp_df_gp[imp_df_gp.imporatance_gain == 0].feature_name.values\n",
        "        feature_cols_with_img = [col for col in feature_cols_with_img if col not in drop_features]\n",
        "        print(len(feature_cols_with_img))\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR / f\"exp{CFG.exp_ver}\", exist_ok=True)\n",
        "    joblib.dump(le_dict, OUTPUT_DIR / f\"exp{CFG.exp_ver}\" / \"labelEncoder.joblib\")\n",
        "\n",
        "    gkf = StratifiedGroupKFold(n_splits=N_SPLITS, random_state=SPLIT_SEED, shuffle=True)\n",
        "    train_df[\"fold\"] = -1\n",
        "    for fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, train_df['target'], groups=train_df[\"patient_id\"])):\n",
        "        train_df.loc[val_idx, \"fold\"] = fold\n",
        "\n",
        "    clfs = []\n",
        "    for fold in range(5):\n",
        "        X = train_df[train_df[\"fold\"] != fold]\n",
        "        y = X[TARGET_COL].astype(float)\n",
        "        X_valid = train_df[train_df[\"fold\"] == fold]\n",
        "        y_valid = X_valid[TARGET_COL].astype(float)\n",
        "\n",
        "        # Indeterminate condition weight adjustment\n",
        "        X['sample_weight'] = 1.0\n",
        "        X_valid['sample_weight'] = 1.0\n",
        "        if CFG.lesion_id_weight > 0:\n",
        "            X.loc[X['lesion_id'].notnull() & (X['target'] == 0), 'sample_weight'] = CFG.lesion_id_weight\n",
        "            X_valid.loc[X_valid['lesion_id'].notnull() & (X_valid['target'] == 0), 'sample_weight'] = CFG.lesion_id_weight\n",
        "\n",
        "        if CFG.Indeterminate_weight > 0:\n",
        "            X.loc[X['iddx_1'] == 'Indeterminate', 'target'] = 1\n",
        "            X.loc[X['iddx_1'] == 'Indeterminate', 'sample_weight'] = 0.5\n",
        "            X_valid.loc[X_valid['iddx_1'] == 'Indeterminate', 'target'] = 1\n",
        "            X_valid.loc[X_valid['iddx_1'] == 'Indeterminate', 'sample_weight'] = 0.5\n",
        "\n",
        "        lgbm_trainer = Trainer(X, y, X_valid, y_valid, params, feature_cols_with_img)\n",
        "        lgbm_trainer.fit()\n",
        "        clfs.append(lgbm_trainer.clf)\n",
        "        lgbm_trainer.clf.save_model(OUTPUT_DIR / f'exp{CFG.exp_ver}' / f\"lgb_fold_{fold}.json\")\n",
        "\n",
        "    # OOF (Out of Fold) predictions\n",
        "    oof_df = train_df[['isic_id', TARGET_COL]].copy()\n",
        "    oof_scores = []\n",
        "    for fold in range(5):\n",
        "        X = train_df[train_df[\"fold\"] != fold]\n",
        "        X_valid = train_df[train_df[\"fold\"] == fold]\n",
        "        tmp_lgb_clf = lgb.Booster(model_file=OUTPUT_DIR / f'exp{CFG.exp_ver}' / f\"lgb_fold_{fold}.json\")\n",
        "        tmp = tmp_lgb_clf.predict(X_valid[feature_cols_with_img])\n",
        "        print(score(X_valid[TARGET_COL], tmp))\n",
        "        oof_scores.append(score(X_valid[TARGET_COL], tmp))\n",
        "        oof_df.loc[X_valid.index, 'pred'] = tmp\n",
        "\n",
        "    print(np.mean(oof_scores))\n",
        "    param_sheet.update_acell(f\"J{int(CFG.exp_ver)+1}\", np.mean(oof_scores))\n",
        "    print(score(oof_df[TARGET_COL], oof_df['pred']))\n",
        "    param_sheet.update_acell(f\"K{int(CFG.exp_ver)+1}\", score(oof_df[TARGET_COL], oof_df['pred']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da42bae-9946-4b5f-9bc4-c4d38d375dd0",
      "metadata": {
        "id": "4da42bae-9946-4b5f-9bc4-c4d38d375dd0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
